{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/behzad/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/behzad/.local/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from os import listdir\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "tqdm.pandas()\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('movie_reviews')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Processor :\n",
    "    def __init__(self, path=None, test_percent=20) :\n",
    "        if path==None :\n",
    "            self.data = None\n",
    "            return\n",
    "        data = {'name':list(), 'txt':list(), 'lbl':list()}\n",
    "        clasdic = {'pos':'+', 'neg':'-'}\n",
    "        for clas in ['pos','neg'] :\n",
    "            files = listdir(path+clas)\n",
    "            tr = trange(len(files), leave=True)\n",
    "            for i in tr :\n",
    "                file = files[i]\n",
    "                data['name'].append(files[i])\n",
    "                data['txt'].append(open(path+clas+'/'+file).read())\n",
    "                data['lbl'].append(clasdic[clas])\n",
    "                tr.set_description(path+clas+'/'+file)\n",
    "        self.data = pd.DataFrame({\n",
    "            'name' : data['name'],\n",
    "            'text' : data['txt'],\n",
    "            'label' : data['lbl']\n",
    "        })\n",
    "        self.pos_idx = (self.data.label == clasdic['pos'])\n",
    "        self.neg_idx = (self.data.label == clasdic['neg'])\n",
    "        self.training_cols = list()\n",
    "                \n",
    "    def process_text(self, text) :\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        t = text\n",
    "        t = t.strip()\n",
    "        t = t.lower()\n",
    "        for p,f in {\"'s\":'is',\"'re\":'are',\"n't\":'not',\"'d\":'had',\"'m\":'am',\"'ve\":'have'}.items() :\n",
    "            t = t.replace(p,' '+f)\n",
    "        for c in np.unique(list(t)).tolist() :\n",
    "            if c.isalpha() :\n",
    "                continue\n",
    "            t = t.replace(c,' ')\n",
    "        t = nltk.tokenize.word_tokenize(t)\n",
    "        t = np.array(t)\n",
    "        t = t[np.logical_or(~np.isin(t,list(ENGLISH_STOP_WORDS)), np.isin(t,['not','no']))]\n",
    "        t = list(map(lemmatizer.lemmatize, t))\n",
    "        n = np.isin(t,['not','no'])\n",
    "        n = np.array([False]+n.tolist()[:-1])\n",
    "        for i,w in enumerate(n) :\n",
    "            if w :\n",
    "                t[i] = 'not-'+t[i]\n",
    "        return t\n",
    "        \n",
    "    def process(self) :\n",
    "        self.data['clean_text'] = self.data.text.progress_apply(self.process_text)\n",
    "        w,c = np.unique(flatten(self.data['clean_text']), return_counts=True)\n",
    "        self.vocab = pd.DataFrame({'word':w, 'counts':c})\n",
    "        self.vocab.set_index('word', inplace=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def copy(cls, other) :\n",
    "        tp = cls()\n",
    "        tp.data = other.data.copy()\n",
    "        tp.vocab = other.vocab.copy()\n",
    "        tp.pos_idx = other.pos_idx\n",
    "        tp.neg_idx = other.neg_idx\n",
    "        tp.training_cols = other.training_cols\n",
    "        return tp\n",
    "\n",
    "    def add_training_col(self, col) :\n",
    "        if 'f_'+col in self.training_cols :\n",
    "            return\n",
    "        self.training_cols.append('f_'+col)\n",
    "    \n",
    "    def add_BOW(self, word) :\n",
    "        self.data['f_'+word] = self.data.clean_text.apply(lambda x: (np.array(x)==word).sum())    \n",
    "        \n",
    "    def make_BOW(self, min_count=10, set_as_feature=False) :\n",
    "        self.BOW_min_count = min_count\n",
    "        list(map(self.add_BOW, tqdm(self.vocab[self.vocab.counts >= min_count].index)))\n",
    "        if set_as_feature :\n",
    "            list(map(self.add_training_col, tqdm(self.vocab[self.vocab.counts >= min_count].index)))\n",
    "        \n",
    "    def make_w2v(self, size=25, window=5, min_count=3) :\n",
    "        self.w2vsize = size\n",
    "        self.w2v_min_count = min_count\n",
    "        self.w2v = Word2Vec(self.data.clean_text, size=size, window=window, min_count=min_count)\n",
    "    \n",
    "    def sen2vec(self, sen) :\n",
    "        words = np.array(sen)\n",
    "        words = words[self.vocab.counts[words] >= self.w2v_min_count]\n",
    "        if words.shape[0] == 0 :\n",
    "            return np.zeros(self.w2vsize)\n",
    "        vec = self.w2v.wv[words].mean(axis=0)\n",
    "        return vec\n",
    "    \n",
    "    def learn_goodnes(self, min_count=100, min_diff=50) :\n",
    "        if min_count == None :\n",
    "            min_count = self.BOW_min_count\n",
    "        if min_count < self.BOW_min_count :\n",
    "            print('goodnes min count can not be less than BOW min count:',self.BOW_min_count)\n",
    "            return\n",
    "        v = list()\n",
    "        p = list()\n",
    "        self.goodnes_vocab = set()\n",
    "        for w in tqdm(self.w2v.wv.vocab) :\n",
    "            if prtp.vocab.loc[w][0] < self.BOW_min_count :\n",
    "                continue\n",
    "            value = self.data['f_'+w][tp.pos_idx].sum() - self.data['f_'+w][tp.neg_idx].sum()\n",
    "            if np.abs(value) < min_diff :\n",
    "                continue\n",
    "            v.append(tp.w2v.wv[w])\n",
    "            p.append(value)\n",
    "            self.goodnes_vocab.add(w)\n",
    "        v = np.array(v)\n",
    "        p = np.array(p)\n",
    "        self.goodnes = LinearRegression()\n",
    "        self.goodnes.fit(v, p)\n",
    "        print(\"goodnes score :\", self.goodnes.score(v,p))\n",
    "        self.goodnes_default = p.mean()\n",
    "    \n",
    "    def how_good(self, word) :\n",
    "        if word not in self.w2v.wv.vocab :\n",
    "            return 'ne'\n",
    "        return self.goodnes.predict([self.w2v.wv[word]])\n",
    "    \n",
    "    def how_good_sen(self, sen) :\n",
    "        words = np.array(sen)\n",
    "        words = np.array([w for w in words if w in self.goodnes_vocab])\n",
    "        if words.shape[0] == 0 :\n",
    "            return 0\n",
    "        return self.goodnes.predict(self.w2v.wv[words]).mean()\n",
    "    \n",
    "    def set_goodnes_values(self, set_as_feature=False) :\n",
    "        self.data['goodnes_value'] = \\\n",
    "            list(map(lambda x: self.how_good_sen(x).mean(), tqdm(self.data.clean_text)))\n",
    "        if set_as_feature :\n",
    "            self.add_training_col('f_goodnes_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./movie_reviews/pos/cv348_18176.txt: 100%|██████████| 1000/1000 [00:02<00:00, 341.58it/s]\n",
      "./movie_reviews/neg/cv669_24318.txt: 100%|██████████| 1000/1000 [00:02<00:00, 356.49it/s]\n",
      "100%|██████████| 2000/2000 [00:18<00:00, 108.43it/s]\n",
      "  1%|          | 26/3663 [00:05<13:45,  4.41it/s]"
     ]
    }
   ],
   "source": [
    "prtp = Text_Processor('./movie_reviews/')\n",
    "prtp.process()\n",
    "prtp.make_BOW(min_count=30, set_as_feature=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = MultinomialNB()\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score,pos_label='+'),\n",
    "           'recall' : make_scorer(recall_score,pos_label='+'), \n",
    "           'f1_score' : make_scorer(f1_score,pos_label='+')}\n",
    "y = prtp.data.label.copy()\n",
    "validate = cross_validate(estimator=estimator,\n",
    "                X=prtp.data[prtp.training_cols]>0,\n",
    "                y=y,\n",
    "                cv=k_fold,\n",
    "                scoring=scoring)\n",
    "validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in validate.items() :\n",
    "    print(key,\":\\t\",val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prtp.make_w2v()\n",
    "prtp.learn_goodnes()\n",
    "prtp.set_goodnes_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = prtp.data['goodnes_value'][:1000]\n",
    "n = prtp.data['goodnes_value'][1000:]\n",
    "plt.hist(p,bins=100,label='Positive', color='g')\n",
    "plt.hist(n,bins=100,label='Negative', color='r')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Goodnes Point')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
